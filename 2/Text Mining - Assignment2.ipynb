{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__*Class*__: Text Minig[BAT54201]\n",
    "\n",
    "__*Professor*__: Lee, Junghye\n",
    "\n",
    "__*Team*__: DW(Don't Worry)\n",
    "\n",
    "__*Member*__: Cha, Ukhyeon(20176022) / Lee, Doyeon(20176026)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction and Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is to understand pLSA and EM algorithms.\n",
    "\n",
    "- Understand the algorithm and implement it yourself.\n",
    "\n",
    "- Create a class using the functions created in Assignment 1. Becase of programformating compile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will attatch doucments handwritten 'Derive PLSA'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# External API\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Part of External API\n",
    "from string import punctuation\n",
    "from operator import itemgetter\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Own code API\n",
    "import TextminingPlsa\n",
    "from utils import normalize\n",
    "from stopwords import stopwords\n",
    "from replacer import RegexpReplacer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Part of Preprocessing Class in Python*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = TextminingPlsa.Corpus()  # instantiate corpus\n",
    "document_paths = ['./test/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Function of making a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_corpus(self):\n",
    "    symbols = punctuation.replace('\\'', '')\n",
    "    self.file = open(self.filepath)\n",
    "    try:\n",
    "        #self.lines = [line for line in self.file]\n",
    "        content = self.file.read().strip()\n",
    "        for symbol in symbols:\n",
    "            content = content.replace(symbol, '')\n",
    "        self.words = self.words + content.split()\n",
    "\n",
    "    finally:\n",
    "        self.file.close()\n",
    "\n",
    "    return self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\\k1.txt\n",
      "./test\\k2.txt\n",
      "./test\\k3.txt\n",
      "./test\\k4.txt\n",
      "./test\\k5.txt\n",
      "./test\\N1.txt\n",
      "./test\\N2.txt\n",
      "./test\\N3.txt\n",
      "./test\\N4.txt\n",
      "./test\\N5.txt\n"
     ]
    }
   ],
   "source": [
    "for document_path in document_paths:\n",
    "    for document_file in glob.glob(os.path.join(document_path, '*.txt')):\n",
    "        print(document_file)\n",
    "        document = TextminingPlsa.PreprocessingCorpus(document_file)  # instantiate document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus:  802\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of corpus: \",len(document.create_corpus()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\\k1.txt\n",
      "['South', 'Korean', 'adults', 'run', 'the', 'risk', 'of', 'chronic', 'diseases', 'due']\n",
      "./test\\k2.txt\n",
      "['A', 'local', 'research', 'team', 'has', 'discovered', 'a', 'form', 'of', 'nanostructure']\n",
      "./test\\k3.txt\n",
      "['Some', '41', 'percent', 'of', 'adult', 'men', 'are', 'overweight', 'much', 'higher']\n",
      "./test\\k4.txt\n",
      "['While', 'millions', 'of', 'people', 'die', 'of', 'hunger', 'in', 'developing', 'countries']\n",
      "./test\\k5.txt\n",
      "['The', 'term', 'babyface', 'has', 'far', 'wider', 'connotations', 'in', 'the', 'modern']\n",
      "./test\\N1.txt\n",
      "['In', 'response', 'to', 'growing', 'consumer', 'frustration', 'over', 'drug', 'prices', 'UnitedHealthcare']\n",
      "./test\\N2.txt\n",
      "['On', 'Sunday', 'The', 'Times', 'published', 'a', 'heartbreaking', 'story', 'about', 'a']\n",
      "./test\\N3.txt\n",
      "['In', 'the', 'wake', 'of', 'the', 'horrific', 'school', 'shootings', 'in', 'Parkland']\n",
      "./test\\N4.txt\n",
      "['When', 'The', 'New', 'York', 'Times', 'hired', 'me', 'to', 'write', 'about']\n",
      "./test\\N5.txt\n",
      "[\"It's\", 'the', 'No', '2', 'killer', 'among', 'illicit', 'drugs', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "for document_path in document_paths:\n",
    "    for document_file in glob.glob(os.path.join(document_path, '*.txt')):\n",
    "        print(document_file)\n",
    "        document = TextminingPlsa.PreprocessingCorpus(document_file)\n",
    "        print(document.create_corpus()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Function of spliting apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unset_apostrophe(self):\n",
    "    \"\"\"\n",
    "    This function needs to import RegexpReplacer.\n",
    "    \"\"\"\n",
    "    replacer = RegexpReplacer()\n",
    "\n",
    "    self.unset_apostrophe_list = []\n",
    "\n",
    "    for element in self.words:\n",
    "        temp_elem = replacer.replace(element).split()\n",
    "        self.unset_apostrophe_list.append(temp_elem.replace('\\'', ''))\n",
    "\n",
    "    return self.unset_apostrophe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\\k1.txt\n",
      "['South', 'Korean', 'adults', 'run', 'the', 'risk', 'of', 'chronic', 'diseases', 'due']\n",
      "./test\\k2.txt\n",
      "['A', 'local', 'research', 'team', 'has', 'discovered', 'a', 'form', 'of', 'nanostructure']\n",
      "./test\\k3.txt\n",
      "['Some', '41', 'percent', 'of', 'adult', 'men', 'are', 'overweight', 'much', 'higher']\n",
      "./test\\k4.txt\n",
      "['While', 'millions', 'of', 'people', 'die', 'of', 'hunger', 'in', 'developing', 'countries']\n",
      "./test\\k5.txt\n",
      "['The', 'term', 'babyface', 'has', 'far', 'wider', 'connotations', 'in', 'the', 'modern']\n",
      "./test\\N1.txt\n",
      "['In', 'response', 'to', 'growing', 'consumer', 'frustration', 'over', 'drug', 'prices', 'UnitedHealthcare']\n",
      "./test\\N2.txt\n",
      "['On', 'Sunday', 'The', 'Times', 'published', 'a', 'heartbreaking', 'story', 'about', 'a']\n",
      "./test\\N3.txt\n",
      "['In', 'the', 'wake', 'of', 'the', 'horrific', 'school', 'shootings', 'in', 'Parkland']\n",
      "./test\\N4.txt\n",
      "['When', 'The', 'New', 'York', 'Times', 'hired', 'me', 'to', 'write', 'about']\n",
      "./test\\N5.txt\n",
      "['It is', 'the', 'No', '2', 'killer', 'among', 'illicit', 'drugs', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "for document_path in document_paths:\n",
    "    for document_file in glob.glob(os.path.join(document_path, '*.txt')):\n",
    "        print(document_file)\n",
    "        document = TextminingPlsa.PreprocessingCorpus(document_file)\n",
    "        document.create_corpus()\n",
    "        print(document.unset_apostrophe()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Function of changing lower characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_text(self):\n",
    "    \"\"\"\n",
    "    Converts the text of all imported files to lowercase.\n",
    "    \"\"\"\n",
    "    self.lower_corpus_list = []\n",
    "\n",
    "    for char in self.unset_apostrophe_list:\n",
    "        lower_character = char.lower()\n",
    "        self.lower_corpus_list.append(lower_character)\n",
    "\n",
    "    return self.lower_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\\k1.txt\n",
      "['south', 'korean', 'adults', 'run', 'the', 'risk', 'of', 'chronic', 'diseases', 'due']\n",
      "./test\\k2.txt\n",
      "['a', 'local', 'research', 'team', 'has', 'discovered', 'a', 'form', 'of', 'nanostructure']\n",
      "./test\\k3.txt\n",
      "['some', '41', 'percent', 'of', 'adult', 'men', 'are', 'overweight', 'much', 'higher']\n",
      "./test\\k4.txt\n",
      "['while', 'millions', 'of', 'people', 'die', 'of', 'hunger', 'in', 'developing', 'countries']\n",
      "./test\\k5.txt\n",
      "['the', 'term', 'babyface', 'has', 'far', 'wider', 'connotations', 'in', 'the', 'modern']\n",
      "./test\\N1.txt\n",
      "['in', 'response', 'to', 'growing', 'consumer', 'frustration', 'over', 'drug', 'prices', 'unitedhealthcare']\n",
      "./test\\N2.txt\n",
      "['on', 'sunday', 'the', 'times', 'published', 'a', 'heartbreaking', 'story', 'about', 'a']\n",
      "./test\\N3.txt\n",
      "['in', 'the', 'wake', 'of', 'the', 'horrific', 'school', 'shootings', 'in', 'parkland']\n",
      "./test\\N4.txt\n",
      "['when', 'the', 'new', 'york', 'times', 'hired', 'me', 'to', 'write', 'about']\n",
      "./test\\N5.txt\n",
      "['it is', 'the', 'no', '2', 'killer', 'among', 'illicit', 'drugs', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "for document_path in document_paths:\n",
    "    for document_file in glob.glob(os.path.join(document_path, '*.txt')):\n",
    "        print(document_file)\n",
    "        document = TextminingPlsa.PreprocessingCorpus(document_file)\n",
    "        document.create_corpus()\n",
    "        document.unset_apostrophe()\n",
    "        print(document.lower_text()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Function of removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopword(self):\n",
    "    \"\"\"\n",
    "    This function needs to import stopwords.\n",
    "    Stopwords can be added or removed.\n",
    "    \"\"\"\n",
    "    self.final_list = self.lower_corpus_list.copy()\n",
    "\n",
    "    for word in self.lower_corpus_list:\n",
    "        if word in stopwords:\n",
    "            self.final_list.remove(word)\n",
    "\n",
    "    return self.final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\\k1.txt\n",
      "['south', 'korean', 'adults', 'run', 'risk', 'chronic', 'diseases', 'due', 'unhealthy', 'diet']\n",
      "./test\\k2.txt\n",
      "['local', 'discovered', 'form', 'nanostructure', 'potential', 'later', 'applications', 'cancer', 'treatment', 'joint']\n",
      "./test\\k3.txt\n",
      "['41', 'adult', 'men', 'overweight', 'much', 'higher', '24', 'women', 'according', 'obesity']\n",
      "./test\\k4.txt\n",
      "['die', 'hunger', 'developing', 'countries', 'genetically', 'modified', 'organisms', 'essential', 'tool', 'fight']\n",
      "./test\\k5.txt\n",
      "['term', 'babyface', 'far', 'wider', 'connotations', 'modern', 'era', 'nickname', 'boxers', 'legendary']\n",
      "./test\\N1.txt\n",
      "['response', 'growing', 'consumer', 'frustration', 'drug', 'prices', 'unitedhealthcare', 'one', 'nation is', 'largest']\n",
      "./test\\N2.txt\n",
      "['heartbreaking', 'standout', 'student', 'williams', 'later', 'developed', 'mental', 'problems', 'ended', 'homeless']\n",
      "./test\\N3.txt\n",
      "['wake', 'horrific', 'school', 'shootings', 'parkland', 'fla', 'trump', 'repeatedly', 'building', 'reopening']\n",
      "./test\\N4.txt\n",
      "['hired', 'me', 'write', 'science', '52', 'years', 'ago', 'i', '40', 'pounds']\n",
      "./test\\N5.txt\n",
      "['it is', '2', 'killer', 'among', 'illicit', 'drugs', 'us', 'kills', 'africanamericans', 'heroin']\n"
     ]
    }
   ],
   "source": [
    "for document_path in document_paths:\n",
    "    for document_file in glob.glob(os.path.join(document_path, '*.txt')):\n",
    "        print(document_file)\n",
    "        document = TextminingPlsa.PreprocessingCorpus(document_file)\n",
    "        document.create_corpus()\n",
    "        document.unset_apostrophe()\n",
    "        document.lower_text()\n",
    "        predoc = document.remove_stopword()\n",
    "        print(predoc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*The Part of Algorithm Class in Python*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Adding to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_document(self, document):\n",
    "    '''\n",
    "    Add a document to the corpus.\n",
    "    '''\n",
    "    self.documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\\k1.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n",
      "./test\\k2.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n",
      "./test\\k3.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n",
      "./test\\k4.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n",
      "./test\\k5.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n",
      "./test\\N1.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n",
      "./test\\N2.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n",
      "./test\\N3.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n",
      "./test\\N4.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n",
      "./test\\N5.txt\n",
      "<TextminingPlsa.Corpus object at 0x000001C2361EA9E8>\n"
     ]
    }
   ],
   "source": [
    "for document_path in document_paths:\n",
    "    for document_file in glob.glob(os.path.join(document_path, '*.txt')):\n",
    "        print(document_file)\n",
    "        document = TextminingPlsa.PreprocessingCorpus(document_file)\n",
    "        document.create_corpus()\n",
    "        document.unset_apostrophe()\n",
    "        document.lower_text()\n",
    "        predoc = document.remove_stopword()\n",
    "        predoc\n",
    "\n",
    "        corpus.add_document(predoc)  # push onto corpus documents list\n",
    "        print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Function of making a unique vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(self):\n",
    "    '''\n",
    "    Construct a list of unique words in the corpus.\n",
    "    '''\n",
    "    discrete_set = set()\n",
    "    for document in self.documents:\n",
    "        for word in document:\n",
    "            discrete_set.add(word)\n",
    "    self.vocabulary = list(discrete_set)\n",
    "\n",
    "    return self.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pursued', 'swayed', 'employment', 'friend', 'unitedhealthcare is', 'building', 'deflect', 'depends', 'longerterm', 'torrey', 'argue', '520', 'back', '50', 'united', 'provision', 'bank', 'else', 'obesity', 'apartment', 'wellpracticed', 'europe', 'womans', 'brown', 'articles', 'advocacy', 'gained', 'america is', 'brought', 'founded']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.build_vocabulary()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Implemetation of EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plsa(self, number_of_topics, max_iter):\n",
    "\n",
    "    '''\n",
    "    Model topics.\n",
    "    '''\n",
    "    print(\"EM iteration begins...\")\n",
    "    # Get vocabulary and number of documents.\n",
    "    self.build_vocabulary()\n",
    "    number_of_documents = len(self.documents)\n",
    "    vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "    # build term-doc matrix\n",
    "    self.term_doc_matrix = np.zeros([number_of_documents, vocabulary_size], dtype=np.int)\n",
    "    for d_index, doc in enumerate(self.documents):\n",
    "        term_count = np.zeros(vocabulary_size, dtype=np.int)\n",
    "        for word in doc:\n",
    "            if word in self.vocabulary:\n",
    "                w_index = self.vocabulary.index(word)\n",
    "                term_count[w_index] += 1\n",
    "        self.term_doc_matrix[d_index] = term_count\n",
    "\n",
    "    # Create the counter arrays.\n",
    "    self.document_topic_prob = np.zeros([number_of_documents, number_of_topics], dtype=np.float)  # P(z | d)\n",
    "    self.topic_word_prob = np.zeros([number_of_topics, len(self.vocabulary)], dtype=np.float)  # P(w | z)\n",
    "    self.topic_prob = np.zeros([number_of_documents, len(self.vocabulary), number_of_topics], dtype=np.float)  # P(z | d, w)\n",
    "\n",
    "    # Initialize\n",
    "    print(\"Initializing...\")\n",
    "    # randomly assign values\n",
    "    self.document_topic_prob = np.random.random(size=(number_of_documents, number_of_topics))\n",
    "    for d_index in range(len(self.documents)):\n",
    "        normalize(self.document_topic_prob[d_index])  # normalize for each document\n",
    "    self.topic_word_prob = np.random.random(size=(number_of_topics, len(self.vocabulary)))\n",
    "    for z in range(number_of_topics):\n",
    "        normalize(self.topic_word_prob[z])  # normalize for each topic\n",
    "\n",
    "    # Run the EM algorithm\n",
    "    temp = 0\n",
    "    word_prob_dist= []\n",
    "    topic_prob_dist = []\n",
    "    for iteration in range(max_iter):\n",
    "        print(\"Iteration #\" + str(iteration + 1) + \"...\")\n",
    "        #print(\"===E step===\")\n",
    "        for d_index, document in enumerate(self.documents):\n",
    "            for w_index in range(vocabulary_size):\n",
    "                prob = self.document_topic_prob[d_index, :] * self.topic_word_prob[:, w_index]\n",
    "                if sum(prob) == 0.0:\n",
    "                    print(\"d_index = \" + str(d_index) + \",  w_index = \" + str(w_index))\n",
    "                    print(\"self.document_topic_prob[d_index, :] = \" + str(self.document_topic_prob[d_index, :]))\n",
    "                    print(\"self.topic_word_prob[:, w_index] = \" + str(self.topic_word_prob[:, w_index]))\n",
    "                    print(\"topic_prob[d_index][w_index] = \" + str(prob))\n",
    "                    exit(0)\n",
    "                else:\n",
    "                    normalize(prob)\n",
    "                self.topic_prob[d_index][w_index] = prob\n",
    "        #print(self.topic_prob.shape)\n",
    "\n",
    "        #print(\"===M step===\")\n",
    "        # update P(w | z); word-distribution\n",
    "        for z in range(number_of_topics):\n",
    "            for w_index in range(vocabulary_size):\n",
    "                s = 0\n",
    "                for d_index in range(len(self.documents)):\n",
    "                    count = self.term_doc_matrix[d_index][w_index]\n",
    "                    s = s + count * self.topic_prob[d_index, w_index, z]\n",
    "                self.topic_word_prob[z][w_index] = s\n",
    "            normalize(self.topic_word_prob[z])\n",
    "        #print(self.topic_word_prob.shape)\n",
    "\n",
    "        # update P(z | d); lamda(coverage)\n",
    "        for d_index in range(len(self.documents)):\n",
    "            for z in range(number_of_topics):\n",
    "                s = 0\n",
    "                for w_index in range(vocabulary_size):\n",
    "                    count = self.term_doc_matrix[d_index][w_index]\n",
    "                    s = s + count * self.topic_prob[d_index, w_index, z]\n",
    "                self.document_topic_prob[d_index][z] = s\n",
    "            normalize(self.document_topic_prob[d_index])\n",
    "        #print(self.document_topic_prob.shape)\n",
    "\n",
    "\n",
    "        if abs(self._loglikelihood() - temp) < 0.0001:\n",
    "            break\n",
    "        else:\n",
    "            temp = self._loglikelihood()\n",
    "            self.listLoglikelihood.append(temp)\n",
    "            word_prob_dist.append(self.topic_word_prob)\n",
    "            topic_prob_dist.append(self.document_topic_prob)\n",
    "\n",
    "    return (self.listLoglikelihood, word_prob_dist, topic_prob_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM iteration begins...\n",
      "Initializing...\n",
      "Iteration #1...\n",
      "Iteration #2...\n",
      "Iteration #3...\n",
      "Iteration #4...\n",
      "Iteration #5...\n",
      "Iteration #6...\n",
      "Iteration #7...\n",
      "Iteration #8...\n",
      "Iteration #9...\n",
      "Iteration #10...\n",
      "Iteration #11...\n",
      "Iteration #12...\n",
      "Iteration #13...\n",
      "Iteration #14...\n",
      "Iteration #15...\n",
      "Iteration #16...\n",
      "Iteration #17...\n",
      "Iteration #18...\n",
      "Iteration #19...\n",
      "Iteration #20...\n",
      "Iteration #21...\n",
      "Iteration #22...\n",
      "Iteration #23...\n",
      "Iteration #24...\n"
     ]
    }
   ],
   "source": [
    "result = corpus.plsa(number_of_topics=3, max_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Calculating of Log-Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _loglikelihood(self):\n",
    "\n",
    "    log_value = np.log(np.dot(self.document_topic_prob, self.topic_word_prob))\n",
    "    cGivenlamda = self.term_doc_matrix * log_value\n",
    "    sumLoglikelihood = 0\n",
    "    for iter in range(len(self.vocabulary)):\n",
    "        sumLoglikelihood += sum(cGivenlamda)[iter,]\n",
    "\n",
    "    return sumLoglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Visualization of the value of Log-Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualization_likelihod(value_list):\n",
    "    \n",
    "    plt.plot(value_list, c=\"b\", lw=2, ls=\"--\", marker=\"o\", ms=5, mec=\"g\", mew=2, mfc=\"r\")\n",
    "    \n",
    "    plt.xlabel('$Iteration$', fontsize=14)\n",
    "    plt.title('Log-Likelihood for Optimization', fontsize=20)\n",
    "    plt.ylabel('$Value$', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZFMUoISEZFM+v9ybCjntoCitwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c238daf898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualization_likelihod(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Result of Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1930, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pursued</th>\n",
       "      <td>3.916862e-70</td>\n",
       "      <td>8.014609e-128</td>\n",
       "      <td>1.301442e-68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swayed</th>\n",
       "      <td>1.288697e-196</td>\n",
       "      <td>2.299546e-91</td>\n",
       "      <td>7.200265e-68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employment</th>\n",
       "      <td>1.242305e-91</td>\n",
       "      <td>4.243206e-130</td>\n",
       "      <td>1.349714e-67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friend</th>\n",
       "      <td>1.480998e-68</td>\n",
       "      <td>6.747030e-91</td>\n",
       "      <td>2.396932e-83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unitedhealthcare is</th>\n",
       "      <td>1.566290e-70</td>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>1.248439e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>9.632582e-197</td>\n",
       "      <td>2.547568e-96</td>\n",
       "      <td>1.213727e-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deflect</th>\n",
       "      <td>8.739076e-03</td>\n",
       "      <td>1.032657e-196</td>\n",
       "      <td>6.532997e-129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depends</th>\n",
       "      <td>5.514767e-68</td>\n",
       "      <td>5.549857e-108</td>\n",
       "      <td>1.938838e-128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longerterm</th>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>3.915988e-69</td>\n",
       "      <td>8.243440e-128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torrey</th>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>8.609882e-197</td>\n",
       "      <td>8.099969e-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>argue</th>\n",
       "      <td>2.959033e-96</td>\n",
       "      <td>4.576470e-68</td>\n",
       "      <td>7.570349e-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1.404508e-195</td>\n",
       "      <td>1.760526e-128</td>\n",
       "      <td>5.215133e-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back</th>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>2.654842e-91</td>\n",
       "      <td>1.504168e-83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>8.502790e-69</td>\n",
       "      <td>5.655424e-75</td>\n",
       "      <td>3.745318e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>united</th>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>2.358684e-69</td>\n",
       "      <td>4.114108e-96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provision</th>\n",
       "      <td>5.391257e-71</td>\n",
       "      <td>3.745318e-03</td>\n",
       "      <td>1.248439e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bank</th>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>3.463524e-69</td>\n",
       "      <td>7.891444e-97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>else</th>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>2.921786e-196</td>\n",
       "      <td>1.981260e-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obesity</th>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>4.785735e-128</td>\n",
       "      <td>3.410183e-72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apartment</th>\n",
       "      <td>2.029511e-196</td>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>3.936555e-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wellpracticed</th>\n",
       "      <td>7.373360e-69</td>\n",
       "      <td>3.745318e-03</td>\n",
       "      <td>7.367271e-96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>europe</th>\n",
       "      <td>1.451478e-91</td>\n",
       "      <td>2.705919e-128</td>\n",
       "      <td>3.171322e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>womans</th>\n",
       "      <td>2.830844e-68</td>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>1.248439e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>1.753363e-69</td>\n",
       "      <td>2.255002e-96</td>\n",
       "      <td>2.496879e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>articles</th>\n",
       "      <td>5.255317e-196</td>\n",
       "      <td>1.544072e-196</td>\n",
       "      <td>6.855210e-97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advocacy</th>\n",
       "      <td>1.248439e-03</td>\n",
       "      <td>9.317675e-92</td>\n",
       "      <td>3.798286e-128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gained</th>\n",
       "      <td>4.202789e-197</td>\n",
       "      <td>4.800979e-128</td>\n",
       "      <td>1.951217e-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america is</th>\n",
       "      <td>1.264021e-74</td>\n",
       "      <td>2.978681e-70</td>\n",
       "      <td>2.986481e-128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brought</th>\n",
       "      <td>1.668683e-81</td>\n",
       "      <td>1.437534e-68</td>\n",
       "      <td>1.350974e-128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>founded</th>\n",
       "      <td>1.886772e-197</td>\n",
       "      <td>1.339122e-68</td>\n",
       "      <td>3.533334e-130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losing</th>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>9.727544e-140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correlation</th>\n",
       "      <td>1.294498e-03</td>\n",
       "      <td>6.330571e-103</td>\n",
       "      <td>2.588997e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.889419e-140</td>\n",
       "      <td>3.658304e-149</td>\n",
       "      <td>1.294498e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outdated</th>\n",
       "      <td>2.199277e-177</td>\n",
       "      <td>2.386005e-177</td>\n",
       "      <td>6.660180e-205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease</th>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>1.294498e-03</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>1.343526e-103</td>\n",
       "      <td>2.877177e-204</td>\n",
       "      <td>4.175966e-104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasting</th>\n",
       "      <td>5.406932e-139</td>\n",
       "      <td>1.294498e-03</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>academy</th>\n",
       "      <td>3.039005e-104</td>\n",
       "      <td>3.236246e-03</td>\n",
       "      <td>1.294498e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>5.496056e-140</td>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>3.607712e-103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>living</th>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>1.294498e-03</td>\n",
       "      <td>8.560282e-178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>importantly</th>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>1.941748e-03</td>\n",
       "      <td>1.294498e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>9.134334e-103</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semblance</th>\n",
       "      <td>9.298425e-104</td>\n",
       "      <td>1.025500e-137</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immediacy</th>\n",
       "      <td>6.472491e-04</td>\n",
       "      <td>3.919977e-103</td>\n",
       "      <td>3.236246e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awoke</th>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>2.610230e-140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addiction</th>\n",
       "      <td>1.294498e-03</td>\n",
       "      <td>5.500451e-139</td>\n",
       "      <td>6.118084e-178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occurred</th>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>1.407283e-204</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are not</th>\n",
       "      <td>8.654703e-103</td>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>2.156250e-138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approach</th>\n",
       "      <td>1.465268e-100</td>\n",
       "      <td>6.472491e-04</td>\n",
       "      <td>1.815967e-138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extremely</th>\n",
       "      <td>8.327256e-109</td>\n",
       "      <td>4.873924e-205</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illicit</th>\n",
       "      <td>3.031269e-103</td>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychosis</th>\n",
       "      <td>2.960090e-103</td>\n",
       "      <td>1.020806e-149</td>\n",
       "      <td>5.825242e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>still</th>\n",
       "      <td>6.472488e-04</td>\n",
       "      <td>1.294498e-03</td>\n",
       "      <td>1.843805e-149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diseases</th>\n",
       "      <td>5.821297e-138</td>\n",
       "      <td>6.472492e-04</td>\n",
       "      <td>1.063990e-137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jail</th>\n",
       "      <td>7.565107e-102</td>\n",
       "      <td>7.175419e-105</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risk</th>\n",
       "      <td>1.097886e-137</td>\n",
       "      <td>8.842306e-105</td>\n",
       "      <td>1.826350e-138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>1.415154e-178</td>\n",
       "      <td>3.883495e-03</td>\n",
       "      <td>6.472490e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far</th>\n",
       "      <td>2.588996e-03</td>\n",
       "      <td>6.472490e-04</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharply</th>\n",
       "      <td>1.613239e-177</td>\n",
       "      <td>6.472488e-04</td>\n",
       "      <td>6.472492e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1930 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Topic1         Topic2         Topic3\n",
       "pursued               3.916862e-70  8.014609e-128   1.301442e-68\n",
       "swayed               1.288697e-196   2.299546e-91   7.200265e-68\n",
       "employment            1.242305e-91  4.243206e-130   1.349714e-67\n",
       "friend                1.480998e-68   6.747030e-91   2.396932e-83\n",
       "unitedhealthcare is   1.566290e-70   1.248439e-03   1.248439e-03\n",
       "building             9.632582e-197   2.547568e-96   1.213727e-69\n",
       "deflect               8.739076e-03  1.032657e-196  6.532997e-129\n",
       "depends               5.514767e-68  5.549857e-108  1.938838e-128\n",
       "longerterm            1.248439e-03   3.915988e-69  8.243440e-128\n",
       "torrey                1.248439e-03  8.609882e-197   8.099969e-69\n",
       "argue                 2.959033e-96   4.576470e-68   7.570349e-69\n",
       "520                  1.404508e-195  1.760526e-128   5.215133e-69\n",
       "back                  1.248439e-03   2.654842e-91   1.504168e-83\n",
       "50                    8.502790e-69   5.655424e-75   3.745318e-03\n",
       "united                1.248439e-03   2.358684e-69   4.114108e-96\n",
       "provision             5.391257e-71   3.745318e-03   1.248439e-03\n",
       "bank                  1.248439e-03   3.463524e-69   7.891444e-97\n",
       "else                  1.248439e-03  2.921786e-196   1.981260e-69\n",
       "obesity               1.248439e-03  4.785735e-128   3.410183e-72\n",
       "apartment            2.029511e-196   1.248439e-03   3.936555e-69\n",
       "wellpracticed         7.373360e-69   3.745318e-03   7.367271e-96\n",
       "europe                1.451478e-91  2.705919e-128   3.171322e-91\n",
       "womans                2.830844e-68   1.248439e-03   1.248439e-03\n",
       "brown                 1.753363e-69   2.255002e-96   2.496879e-03\n",
       "articles             5.255317e-196  1.544072e-196   6.855210e-97\n",
       "advocacy              1.248439e-03   9.317675e-92  3.798286e-128\n",
       "gained               4.202789e-197  4.800979e-128   1.951217e-69\n",
       "america is            1.264021e-74   2.978681e-70  2.986481e-128\n",
       "brought               1.668683e-81   1.437534e-68  1.350974e-128\n",
       "founded              1.886772e-197   1.339122e-68  3.533334e-130\n",
       "...                            ...            ...            ...\n",
       "losing                6.472492e-04   6.472492e-04  9.727544e-140\n",
       "correlation           1.294498e-03  6.330571e-103   2.588997e-03\n",
       "0                    1.889419e-140  3.658304e-149   1.294498e-03\n",
       "outdated             2.199277e-177  2.386005e-177  6.660180e-205\n",
       "disease               6.472492e-04   1.294498e-03   6.472492e-04\n",
       "senior               1.343526e-103  2.877177e-204  4.175966e-104\n",
       "lasting              5.406932e-139   1.294498e-03   6.472492e-04\n",
       "academy              3.039005e-104   3.236246e-03   1.294498e-03\n",
       "like                 5.496056e-140   6.472492e-04  3.607712e-103\n",
       "end                   6.472492e-04   6.472492e-04   6.472492e-04\n",
       "living                6.472492e-04   1.294498e-03  8.560282e-178\n",
       "importantly           6.472492e-04   1.941748e-03   1.294498e-03\n",
       "problem               6.472492e-04  9.134334e-103   6.472492e-04\n",
       "semblance            9.298425e-104  1.025500e-137   6.472492e-04\n",
       "immediacy             6.472491e-04  3.919977e-103   3.236246e-03\n",
       "awoke                 6.472492e-04   6.472492e-04  2.610230e-140\n",
       "addiction             1.294498e-03  5.500451e-139  6.118084e-178\n",
       "occurred              6.472492e-04  1.407283e-204   6.472492e-04\n",
       "are not              8.654703e-103   6.472492e-04  2.156250e-138\n",
       "approach             1.465268e-100   6.472491e-04  1.815967e-138\n",
       "extremely            8.327256e-109  4.873924e-205   6.472492e-04\n",
       "illicit              3.031269e-103   6.472492e-04   6.472492e-04\n",
       "psychosis            2.960090e-103  1.020806e-149   5.825242e-03\n",
       "still                 6.472488e-04   1.294498e-03  1.843805e-149\n",
       "diseases             5.821297e-138   6.472492e-04  1.063990e-137\n",
       "jail                 7.565107e-102  7.175419e-105   6.472492e-04\n",
       "risk                 1.097886e-137  8.842306e-105  1.826350e-138\n",
       "white                1.415154e-178   3.883495e-03   6.472490e-04\n",
       "far                   2.588996e-03   6.472490e-04   6.472492e-04\n",
       "sharply              1.613239e-177   6.472488e-04   6.472492e-04\n",
       "\n",
       "[1930 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_topic = DataFrame(result[1][0].reshape([1930,3]))\n",
    "word_topic.index = corpus.build_vocabulary()\n",
    "word_topic.columns = ['Topic1', 'Topic2', 'Topic3']\n",
    "\n",
    "print(word_topic.shape)\n",
    "word_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic1    1.028039\n",
       "Topic2    0.957011\n",
       "Topic3    1.014950\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(word_topic, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doucment1</th>\n",
       "      <th>Document2</th>\n",
       "      <th>Document3</th>\n",
       "      <th>Document4</th>\n",
       "      <th>Document5</th>\n",
       "      <th>Document6</th>\n",
       "      <th>Document7</th>\n",
       "      <th>Document8</th>\n",
       "      <th>Document9</th>\n",
       "      <th>Document10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>1.306839e-07</td>\n",
       "      <td>8.902960e-10</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>3.115586e-10</td>\n",
       "      <td>2.437751e-11</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.590987e-19</td>\n",
       "      <td>3.034947e-19</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>7.573748e-15</td>\n",
       "      <td>3.624325e-15</td>\n",
       "      <td>1.295509e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.708817e-12</td>\n",
       "      <td>1.923875e-10</td>\n",
       "      <td>4.423277e-13</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.947763e-18</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>3.880155e-12</td>\n",
       "      <td>2.341895e-09</td>\n",
       "      <td>9.209882e-08</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>6.064544e-13</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.439244e-12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.456818e-15</td>\n",
       "      <td>6.972458e-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Doucment1     Document2     Document3     Document4     Document5  \\\n",
       "Topic1  1.306839e-07  8.902960e-10  9.999999e-01  3.115586e-10  2.437751e-11   \n",
       "Topic2  7.573748e-15  3.624325e-15  1.295509e-10  1.000000e+00  3.708817e-12   \n",
       "Topic3  3.880155e-12  2.341895e-09  9.209882e-08  9.999999e-01  6.064544e-13   \n",
       "\n",
       "           Document6     Document7     Document8     Document9    Document10  \n",
       "Topic1  1.000000e+00  1.000000e+00  4.590987e-19  3.034947e-19  1.000000e+00  \n",
       "Topic2  1.923875e-10  4.423277e-13  1.000000e+00  5.947763e-18  1.000000e+00  \n",
       "Topic3  1.000000e+00  7.439244e-12  1.000000e+00  3.456818e-15  6.972458e-15  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_doc = DataFrame(result[2][0].reshape(3,10))\n",
    "topic_doc.index = ['Topic1', 'Topic2', 'Topic3']\n",
    "topic_doc.columns = ['Doucment1','Document2', 'Document3',\n",
    "                   'Document4','Document5', 'Document6',\n",
    "                   'Document7','Document8', 'Document9', 'Document10']\n",
    "\n",
    "print(topic_doc.shape)\n",
    "topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doucment1     1.306878e-07\n",
       "Document2     3.232195e-09\n",
       "Document3     1.000000e+00\n",
       "Document4     2.000000e+00\n",
       "Document5     2.869278e-11\n",
       "Document6     2.000000e+00\n",
       "Document7     1.000000e+00\n",
       "Document8     2.000000e+00\n",
       "Document9     3.463069e-15\n",
       "Document10    2.000000e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(topic_doc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930\n"
     ]
    }
   ],
   "source": [
    "print(len(result[1][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_data ={\n",
    "    '0.Word':[corpus.build_vocabulary()[0],corpus.build_vocabulary()[1],corpus.build_vocabulary()[2],'Log-Likelihood'],\n",
    "    '01.Iteration_P(w|topic1)': [result[1][0][0][0],result[1][0][0][1],result[1][0][0][2],result[0][1]],\n",
    "    '02.Iteration_P(w|topic1)': [result[1][1][0][0],result[1][1][0][1],result[1][1][0][2],result[0][2]],\n",
    "    '03.Iteration_P(w|topic1)': [result[1][2][0][0],result[1][2][0][1],result[1][2][0][2],result[0][3]],\n",
    "    '10.Iteration.....': ['-','-','-','-'],\n",
    "    '25.Iteration_P(w|topic1)': [result[1][-3][0][0],result[1][-3][0][1],result[1][-3][0][2],result[0][-3]],\n",
    "    '26.Iteration_P(w|topic1)': [result[1][-2][0][0],result[1][-2][0][1],result[1][-2][0][2],result[0][-2]],\n",
    "    '27.Iteration_P(w|topic1)': [result[1][-1][0][0],result[1][-1][0][1],result[1][-1][0][2],result[0][-1]],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.Word</th>\n",
       "      <th>01.Iteration_P(w|topic1)</th>\n",
       "      <th>02.Iteration_P(w|topic1)</th>\n",
       "      <th>03.Iteration_P(w|topic1)</th>\n",
       "      <th>10.Iteration.....</th>\n",
       "      <th>25.Iteration_P(w|topic1)</th>\n",
       "      <th>26.Iteration_P(w|topic1)</th>\n",
       "      <th>27.Iteration_P(w|topic1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pursued</td>\n",
       "      <td>3.916862e-70</td>\n",
       "      <td>3.916862e-70</td>\n",
       "      <td>3.916862e-70</td>\n",
       "      <td>-</td>\n",
       "      <td>3.916862e-70</td>\n",
       "      <td>3.916862e-70</td>\n",
       "      <td>3.916862e-70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swayed</td>\n",
       "      <td>8.014609e-128</td>\n",
       "      <td>8.014609e-128</td>\n",
       "      <td>8.014609e-128</td>\n",
       "      <td>-</td>\n",
       "      <td>8.014609e-128</td>\n",
       "      <td>8.014609e-128</td>\n",
       "      <td>8.014609e-128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>employment</td>\n",
       "      <td>1.301442e-68</td>\n",
       "      <td>1.301442e-68</td>\n",
       "      <td>1.301442e-68</td>\n",
       "      <td>-</td>\n",
       "      <td>1.301442e-68</td>\n",
       "      <td>1.301442e-68</td>\n",
       "      <td>1.301442e-68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Log-Likelihood</td>\n",
       "      <td>-2.664017e+04</td>\n",
       "      <td>-2.613215e+04</td>\n",
       "      <td>-2.564053e+04</td>\n",
       "      <td>-</td>\n",
       "      <td>-2.452203e+04</td>\n",
       "      <td>-2.452203e+04</td>\n",
       "      <td>-2.452203e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0.Word  01.Iteration_P(w|topic1)  02.Iteration_P(w|topic1)  \\\n",
       "0         pursued              3.916862e-70              3.916862e-70   \n",
       "1          swayed             8.014609e-128             8.014609e-128   \n",
       "2      employment              1.301442e-68              1.301442e-68   \n",
       "3  Log-Likelihood             -2.664017e+04             -2.613215e+04   \n",
       "\n",
       "   03.Iteration_P(w|topic1) 10.Iteration.....  25.Iteration_P(w|topic1)  \\\n",
       "0              3.916862e-70                 -              3.916862e-70   \n",
       "1             8.014609e-128                 -             8.014609e-128   \n",
       "2              1.301442e-68                 -              1.301442e-68   \n",
       "3             -2.564053e+04                 -             -2.452203e+04   \n",
       "\n",
       "   26.Iteration_P(w|topic1)  27.Iteration_P(w|topic1)  \n",
       "0              3.916862e-70              3.916862e-70  \n",
       "1             8.014609e-128             8.014609e-128  \n",
       "2              1.301442e-68              1.301442e-68  \n",
       "3             -2.452203e+04             -2.452203e+04  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2_data = DataFrame(word_data)\n",
    "result2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits & Futher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could not think about the background.\n",
    "\n",
    "- There is an error in preprocessing. (apostrophe: The string was not splited.)\n",
    "\n",
    "- Constraint about pi ($P(z|d)$) NOT satisfied "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For more information, please refer to the following references and soft copy.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">reference[[1]] [site1] PLSA in github\n",
    "[site1]: https://github.com/hitalex/PLSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">reference[[2]] [site2] Understanding for PLSA (blog)\n",
    "[site2]: https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/05/25/plsa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*- End of Document -* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall code (included class of Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextminingPlsa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from string import punctuation\n",
    "from utils import normalize\n",
    "from stopwords import stopwords\n",
    "from replacer import RegexpReplacer\n",
    "\n",
    "\"\"\"\n",
    "Author: \n",
    "Chris Cha (https://github.com/ryan-chris)\n",
    "Reference:\n",
    "https://github.com/hitalex\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def progressBar(value, endvalue, bar_length=20):\n",
    "    percent = float(value) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length) - 1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "class PreprocessingCorpus(object):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.lines = []\n",
    "        self.words = []\n",
    "\n",
    "    def create_corpus(self):\n",
    "        '''\n",
    "        Create a corpus from document.\n",
    "        '''\n",
    "        symbols = punctuation.replace('\\'', '')\n",
    "        self.file = open(self.filepath)\n",
    "        try:\n",
    "            #self.lines = [line for line in self.file]\n",
    "            content = self.file.read().strip()\n",
    "            for symbol in symbols:\n",
    "                content = content.replace(symbol, '')\n",
    "            self.words = self.words + content.split()\n",
    "\n",
    "        finally:\n",
    "            self.file.close()\n",
    "\n",
    "        return self.words\n",
    "    \"\"\"\n",
    "    def create_corpus(self):\n",
    "\n",
    "        # When it reads files, removes all special symbols, including punctuation marks.\n",
    "        symbols = punctuation.replace('\\'', '')\n",
    "\n",
    "        for filename in self.filepath:\n",
    "            with open(filename) as file:\n",
    "                content = file.read().strip()\n",
    "                for symbol in symbols:\n",
    "                    content = content.replace(symbol, '')\n",
    "                self.words = self.words + content.split()\n",
    "\n",
    "        return self.words\n",
    "    \"\"\"\n",
    "\n",
    "    def lower_text(self):\n",
    "        \"\"\"\n",
    "        Converts the text of all imported files to lowercase.\n",
    "        \"\"\"\n",
    "        self.lower_corpus_list = []\n",
    "\n",
    "        for char in self.unset_apostrophe_list:\n",
    "            lower_character = char.lower()\n",
    "            self.lower_corpus_list.append(lower_character)\n",
    "\n",
    "        return self.lower_corpus_list\n",
    "\n",
    "    def unset_apostrophe(self):\n",
    "        \"\"\"\n",
    "        This function needs to import RegexpReplacer.\n",
    "        \"\"\"\n",
    "        replacer = RegexpReplacer()\n",
    "\n",
    "        self.unset_apostrophe_list = []\n",
    "\n",
    "        for element in self.words:\n",
    "            temp_elem = replacer.replace(element)\n",
    "            self.unset_apostrophe_list.append(temp_elem.replace('\\'', ''))\n",
    "\n",
    "        return self.unset_apostrophe_list\n",
    "\n",
    "    def remove_stopword(self):\n",
    "        \"\"\"\n",
    "        This function needs to import stopwords.\n",
    "        Stopwords can be added or removed.\n",
    "        \"\"\"\n",
    "        self.final_list = self.lower_corpus_list.copy()\n",
    "\n",
    "        for word in self.lower_corpus_list:\n",
    "            if word in stopwords:\n",
    "                self.final_list.remove(word)\n",
    "\n",
    "        return self.final_list\n",
    "\n",
    "class Corpus(object):\n",
    "    '''\n",
    "    A collection of documents.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize empty document list.\n",
    "        '''\n",
    "        self.documents = []\n",
    "        self.listLoglikelihood=[]\n",
    "\n",
    "    def add_document(self, document):\n",
    "        '''\n",
    "        Add a document to the corpus.\n",
    "        '''\n",
    "        self.documents.append(document)\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        '''\n",
    "        Construct a list of unique words in the corpus.\n",
    "        '''\n",
    "        # ** ADD ** #\n",
    "        # exclude words that appear in 90%+ of the documents\n",
    "        # exclude words that are too (in)frequent\n",
    "        discrete_set = set()\n",
    "        for document in self.documents:\n",
    "            for word in document:\n",
    "                discrete_set.add(word)\n",
    "        self.vocabulary = list(discrete_set)\n",
    "        #print(self.vocabulary)\n",
    "\n",
    "        return self.vocabulary\n",
    "\n",
    "\n",
    "    def plsa(self, number_of_topics, max_iter):\n",
    "\n",
    "        '''\n",
    "        Model topics.\n",
    "        '''\n",
    "        print(\"EM iteration begins...\")\n",
    "        # Get vocabulary and number of documents.\n",
    "        self.build_vocabulary()\n",
    "        number_of_documents = len(self.documents)\n",
    "        vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "        # build term-doc matrix\n",
    "        self.term_doc_matrix = np.zeros([number_of_documents, vocabulary_size], dtype=np.int)\n",
    "        for d_index, doc in enumerate(self.documents):\n",
    "            term_count = np.zeros(vocabulary_size, dtype=np.int)\n",
    "            for word in doc:\n",
    "                if word in self.vocabulary:\n",
    "                    w_index = self.vocabulary.index(word)\n",
    "                    term_count[w_index] += 1\n",
    "            self.term_doc_matrix[d_index] = term_count\n",
    "\n",
    "        # Create the counter arrays.\n",
    "        self.document_topic_prob = np.zeros([number_of_documents, number_of_topics], dtype=np.float)  # P(z | d)\n",
    "        self.topic_word_prob = np.zeros([number_of_topics, len(self.vocabulary)], dtype=np.float)  # P(w | z)\n",
    "        self.topic_prob = np.zeros([number_of_documents, len(self.vocabulary), number_of_topics], dtype=np.float)  # P(z | d, w)\n",
    "\n",
    "        # Initialize\n",
    "        print(\"Initializing...\")\n",
    "        # randomly assign values\n",
    "        self.document_topic_prob = np.random.random(size=(number_of_documents, number_of_topics))\n",
    "        for d_index in range(len(self.documents)):\n",
    "            normalize(self.document_topic_prob[d_index])  # normalize for each document\n",
    "        self.topic_word_prob = np.random.random(size=(number_of_topics, len(self.vocabulary)))\n",
    "        for z in range(number_of_topics):\n",
    "            normalize(self.topic_word_prob[z])  # normalize for each topic\n",
    "\n",
    "        # Run the EM algorithm\n",
    "        temp = 0\n",
    "        word_prob_dist= []\n",
    "        topic_prob_dist = []\n",
    "        for iteration in range(max_iter):\n",
    "            print(\"Iteration #\" + str(iteration + 1) + \"...\")\n",
    "            #print(\"===E step===\")\n",
    "            for d_index, document in enumerate(self.documents):\n",
    "                for w_index in range(vocabulary_size):\n",
    "                    prob = self.document_topic_prob[d_index, :] * self.topic_word_prob[:, w_index]\n",
    "                    if sum(prob) == 0.0:\n",
    "                        print(\"d_index = \" + str(d_index) + \",  w_index = \" + str(w_index))\n",
    "                        print(\"self.document_topic_prob[d_index, :] = \" + str(self.document_topic_prob[d_index, :]))\n",
    "                        print(\"self.topic_word_prob[:, w_index] = \" + str(self.topic_word_prob[:, w_index]))\n",
    "                        print(\"topic_prob[d_index][w_index] = \" + str(prob))\n",
    "                        exit(0)\n",
    "                    else:\n",
    "                        normalize(prob)\n",
    "                    self.topic_prob[d_index][w_index] = prob\n",
    "            #print(self.topic_prob.shape)\n",
    "\n",
    "            #print(\"===M step===\")\n",
    "            # update P(w | z); word-distribution\n",
    "            for z in range(number_of_topics):\n",
    "                for w_index in range(vocabulary_size):\n",
    "                    s = 0\n",
    "                    for d_index in range(len(self.documents)):\n",
    "                        count = self.term_doc_matrix[d_index][w_index]\n",
    "                        s = s + count * self.topic_prob[d_index, w_index, z]\n",
    "                    self.topic_word_prob[z][w_index] = s\n",
    "                normalize(self.topic_word_prob[z])\n",
    "            #print(self.topic_word_prob.shape)\n",
    "\n",
    "            # update P(z | d); lamda(coverage)\n",
    "            for d_index in range(len(self.documents)):\n",
    "                for z in range(number_of_topics):\n",
    "                    s = 0\n",
    "                    for w_index in range(vocabulary_size):\n",
    "                        count = self.term_doc_matrix[d_index][w_index]\n",
    "                        s = s + count * self.topic_prob[d_index, w_index, z]\n",
    "                    self.document_topic_prob[d_index][z] = s\n",
    "                normalize(self.document_topic_prob[d_index])\n",
    "            #print(self.document_topic_prob.shape)\n",
    "\n",
    "\n",
    "            if abs(self._loglikelihood() - temp) < 0.0001:\n",
    "                break\n",
    "            else:\n",
    "                temp = self._loglikelihood()\n",
    "                self.listLoglikelihood.append(temp)\n",
    "                word_prob_dist.append(self.topic_word_prob)\n",
    "                topic_prob_dist.append(self.document_topic_prob)\n",
    "\n",
    "        return (self.listLoglikelihood, word_prob_dist, topic_prob_dist)\n",
    "\n",
    "\n",
    "    def _loglikelihood(self):\n",
    "\n",
    "        log_value = np.log(np.dot(self.document_topic_prob, self.topic_word_prob))\n",
    "        cGivenlamda = self.term_doc_matrix * log_value\n",
    "        sumLoglikelihood = 0\n",
    "        for iter in range(len(self.vocabulary)):\n",
    "            sumLoglikelihood += sum(cGivenlamda)[iter,]\n",
    "\n",
    "        return sumLoglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main_chris.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# External API\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Part of External API\n",
    "from operator import itemgetter\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Own code API\n",
    "import TextminingPlsa\n",
    "\n",
    "def visualization_likelihod(value_list):\n",
    "    plt.plot(value_list)\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "\n",
    "    corpus = TextminingPlsa.Corpus()  # instantiate corpus\n",
    "    document_paths = ['./test/']\n",
    "\n",
    "\n",
    "    for document_path in document_paths:\n",
    "        for document_file in glob.glob(os.path.join(document_path, '*.txt')):\n",
    "            #print(document_file)\n",
    "            document = TextminingPlsa.PreprocessingCorpus(document_file)  # instantiate document\n",
    "            document.create_corpus()\n",
    "            document.unset_apostrophe()\n",
    "            document.lower_text()\n",
    "            predoc = document.remove_stopword()\n",
    "            corpus.add_document(predoc)  # push onto corpus documents list\n",
    "\n",
    "    corpus.build_vocabulary()\n",
    "\n",
    "    result = corpus.plsa(number_of_topics=5, max_iter=100)\n",
    "    print(result)\n",
    "\n",
    "    visualization_likelihod(result[0])\n",
    "\n",
    "    making_data ={\n",
    "        'Word':[corpus.build_vocabulary()[0],corpus.build_vocabulary()[1],corpus.build_vocabulary()[2],'Log-Likelihood'],\n",
    "        'Iteration1_P(w|topic1)': [result[1][0][0][0],result[1][0][0][1],result[1][0][0][2],result[0][1]],\n",
    "        'Iteration2_P(w|topic1)': [result[1][1][0][0],result[1][1][0][1],result[1][1][0][2],result[0][2]],\n",
    "        'Iteration3_P(w|topic1)': [result[1][2][0][0],result[1][2][0][1],result[1][2][0][2],result[0][3]],\n",
    "        'Iteration4_P(w|topic1)': [result[1][-3][0][0],result[1][-3][0][1],result[1][-3][0][2],result[0][-3]],\n",
    "        'Iteration5_P(w|topic1)': [result[1][-2][0][0],result[1][-2][0][1],result[1][-2][0][2],result[0][-2]],\n",
    "        'Iteration6_P(w|topic1)': [result[1][-1][0][0],result[1][-1][0][1],result[1][-1][0][2],result[0][-1]],\n",
    "    }\n",
    "\n",
    "    result_data = DataFrame(making_data)\n",
    "    print(result_data)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
